{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装VLLM\n",
    "    pip install vllm -i https://mirrors.aliyun.com/pypi/simple/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import ctypes\n",
    "import torch\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "# Initializing the vLLM model\n",
    "llm = VLLM(\n",
    "    model=\"/input0/Qwen2.5-1.5B-Instruct\",\n",
    "    trust_remote_code=True,  # mandatory for Hugging Face models\n",
    "    max_new_tokens=1024,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "# Running a simple query\n",
    "print(llm.invoke(\"What are the most popular Halloween Costumes?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "awq本地模型可以用过以下方法来载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"TheBloke/Llama-2-7b-Chat-AWQ\", quantization=\"AWQ\")\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APC缓存机制示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "# A prompt containing a large markdown table. The table is randomly generated by GPT-4.\n",
    "LONG_PROMPT = \"You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\\n# Table\\n\" + \"\"\"\n",
    "| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |\n",
    "|-----|---------------|-----|---------------|---------------|------------------------|----------------|------------------------------|\n",
    "| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |\n",
    "| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |\n",
    "| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |\n",
    "| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |\n",
    "| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |\n",
    "| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |\n",
    "| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |\n",
    "| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |\n",
    "| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |\n",
    "| 10  | Henry Violet  | 32  | Artist        | Australia     | henry.v@example.com    | 555-4444       | 753 Willow St, Melbourne, VIC|\n",
    "| 11  | Irene Orange  | 26  | Scientist     | New Zealand   | irene.o@example.com    | 555-5555       | 912 Poplar St, Auckland, NZ  |\n",
    "| 12  | Jack Indigo   | 38  | Teacher       | Ireland       | jack.i@example.com     | 555-6666       | 159 Elm St, Cork, IE         |\n",
    "| 13  | Karen Red     | 41  | Lawyer        | USA           | karen.r@example.com    | 555-7777       | 357 Cedar St, Boston, MA     |\n",
    "| 14  | Leo Brown     | 30  | Chef          | Canada        | leo.b@example.com      | 555-8888       | 246 Oak St, Calgary, AB      |\n",
    "| 15  | Mia Green     | 33  | Musician      | UK            | mia.g@example.com      | 555-9999       | 975 Pine St, Edinburgh, UK   |\n",
    "| 16  | Noah Yellow   | 29  | Doctor        | Australia     | noah.y@example.com     | 555-0000       | 864 Birch St, Brisbane, QLD  |\n",
    "| 17  | Olivia Blue   | 35  | Engineer      | New Zealand   | olivia.b@example.com   | 555-1212       | 753 Maple St, Hamilton, NZ   |\n",
    "| 18  | Peter Black   | 42  | Artist        | Ireland       | peter.b@example.com    | 555-3434       | 912 Fir St, Limerick, IE     |\n",
    "| 19  | Quinn White   | 28  | Scientist     | USA           | quinn.w@example.com    | 555-5656       | 159 Willow St, Seattle, WA   |\n",
    "| 20  | Rachel Red    | 31  | Teacher       | Canada        | rachel.r@example.com   | 555-7878       | 357 Poplar St, Ottawa, ON    |\n",
    "| 21  | Steve Green   | 44  | Lawyer        | UK            | steve.g@example.com    | 555-9090       | 753 Elm St, Birmingham, UK   |\n",
    "| 22  | Tina Blue     | 36  | Musician      | Australia     | tina.b@example.com     | 555-1213       | 864 Cedar St, Perth, WA      |\n",
    "| 23  | Umar Black    | 39  | Chef          | New Zealand   | umar.b@example.com     | 555-3435       | 975 Spruce St, Christchurch, NZ|\n",
    "| 24  | Victor Yellow | 43  | Engineer      | Ireland       | victor.y@example.com   | 555-5657       | 246 Willow St, Galway, IE    |\n",
    "| 25  | Wendy Orange  | 27  | Artist        | USA           | wendy.o@example.com    | 555-7879       | 135 Elm St, Denver, CO       |\n",
    "| 26  | Xavier Green  | 34  | Scientist     | Canada        | xavier.g@example.com   | 555-9091       | 357 Oak St, Montreal, QC     |\n",
    "| 27  | Yara Red      | 41  | Teacher       | UK            | yara.r@example.com     | 555-1214       | 975 Pine St, Leeds, UK       |\n",
    "| 28  | Zack Blue     | 30  | Lawyer        | Australia     | zack.b@example.com     | 555-3436       | 135 Birch St, Adelaide, SA   |\n",
    "| 29  | Amy White     | 33  | Musician      | New Zealand   | amy.w@example.com      | 555-5658       | 159 Maple St, Wellington, NZ |\n",
    "| 30  | Ben Black     | 38  | Chef          | Ireland       | ben.b@example.com      | 555-7870       | 246 Fir St, Waterford, IE    |\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_generation_time(llm, sampling_params, prompts):\n",
    "    # time the generation\n",
    "    start_time = time.time()\n",
    "    output = llm.generate(prompts, sampling_params=sampling_params)\n",
    "    end_time = time.time()\n",
    "    # print the output and generation time\n",
    "    print(f\"Output: {output[0].outputs[0].text}\")\n",
    "    print(f\"Generation time: {end_time - start_time} seconds.\")\n",
    "\n",
    "\n",
    "# set enable_prefix_caching=True to enable APC\n",
    "llm = LLM(\n",
    "    model='lmsys/longchat-13b-16k',\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=100)\n",
    "\n",
    "# Querying the age of John Doe\n",
    "get_generation_time(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    LONG_PROMPT + \"Question: what is the age of John Doe? Your answer: The age of John Doe is \",\n",
    ")\n",
    "\n",
    "# Querying the age of Zack Blue\n",
    "# This query will be faster since vllm avoids computing the KV cache of LONG_PROMPT again.\n",
    "get_generation_time(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    LONG_PROMPT + \"Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is \",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
